{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659973f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 187>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m     ax2\u001b[38;5;241m.\u001b[39mimshow(selected_features, cmap\u001b[38;5;241m=\u001b[39mcolors\u001b[38;5;241m.\u001b[39mListedColormap([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# Read the feature-engineered data into a pandas dataframe\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# Data obtained from http://biostat.mc.vanderbilt.edu/DataSets\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m#demo_path = os.path.dirname(os.path.abspath(__file__))\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     demo_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mabsolute()\n\u001b[0;32m    192\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(demo_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformatted_titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    193\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Copyright 2019 D-Wave Systems Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"agg\")    # must select backend before importing pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# D-Wave Ocean tools\n",
    "import dimod\n",
    "from dwave.system import DWaveCliqueSampler\n",
    "\n",
    "\n",
    "# Define MI calculations\n",
    "def prob(dataset):\n",
    "    \"\"\"Joint probability distribution P(X) for the given data.\"\"\"\n",
    "\n",
    "    # bin by the number of different values per feature\n",
    "    num_rows, num_columns = dataset.shape\n",
    "    bins = [len(np.unique(dataset[:, ci])) for ci in range(num_columns)]\n",
    "\n",
    "    prob, _ = np.histogramdd(dataset, bins)\n",
    "    return prob / np.sum(prob)\n",
    "\n",
    "\n",
    "def shannon_entropy(p):\n",
    "    \"\"\"Shannon entropy H(X) is the negative sum of P(X)log(P(X)) for probability\n",
    "    distribution P(X).\n",
    "    \"\"\"\n",
    "    p = p.flatten()\n",
    "    return -sum(pi*np.log2(pi) for pi in p if pi)\n",
    "\n",
    "\n",
    "def conditional_shannon_entropy(p, *conditional_indices):\n",
    "    \"\"\"Conditional Shannon entropy H(X|Y) = H(X,Y) - H(Y).\"\"\"\n",
    "\n",
    "    # Sanity check on validity of conditional_indices.  In particular,\n",
    "    # try to trap issues in which dimensions have been removed from\n",
    "    # probability table through marginalization, but\n",
    "    # conditional_indices were not updated accordingly.\n",
    "    assert(all(ci < p.ndim for ci in conditional_indices))\n",
    "\n",
    "    axis = tuple(i for i in np.arange(len(p.shape))\n",
    "                 if i not in conditional_indices)\n",
    "\n",
    "    return shannon_entropy(p) - shannon_entropy(np.sum(p, axis=axis))\n",
    "\n",
    "\n",
    "def mutual_information(prob, j):\n",
    "    \"\"\"Mutual information between variables X and variable Y.\n",
    "\n",
    "    Calculated as I(X; Y) = H(X) - H(X|Y).\"\"\"\n",
    "\n",
    "    return (shannon_entropy(np.sum(prob, axis=j))\n",
    "            - conditional_shannon_entropy(prob, j))\n",
    "\n",
    "\n",
    "def conditional_mutual_information(p, j, *conditional_indices):\n",
    "    \"\"\"Mutual information between variables X and variable Y conditional on variable Z.\n",
    "\n",
    "    Calculated as I(X;Y|Z) = H(X|Z) - H(X|Y,Z)\"\"\"\n",
    "\n",
    "    # Compute an updated version of the conditional indices for use\n",
    "    # when the probability table is marginalized over dimension j.\n",
    "    # This marginalization removes one dimension, so any conditional\n",
    "    # indices pointing to dimensions after this one must be adjusted\n",
    "    # accordingly.\n",
    "    marginal_conditional_indices = [i-1 if i > j else i for i in conditional_indices]\n",
    "\n",
    "    return (conditional_shannon_entropy(np.sum(p, axis=j), *marginal_conditional_indices)\n",
    "            - conditional_shannon_entropy(p, j, *conditional_indices))\n",
    "\n",
    "\n",
    "def maximum_energy_delta(bqm):\n",
    "    \"\"\"Compute conservative bound on maximum change in energy when flipping a single variable\"\"\"\n",
    "    return max(abs(bqm.get_linear(i))\n",
    "               + sum(abs(bqm.get_quadratic(i,j))\n",
    "                     for j, _ in bqm.iter_neighborhood(i))\n",
    "               for i in bqm.variables)\n",
    "\n",
    "\n",
    "def mutual_information_bqm(dataset, features, target):\n",
    "    \"\"\"Build a BQM that maximizes MI between survival and a subset of features\"\"\"\n",
    "    variables = ((feature, -mutual_information(prob(dataset[[target, feature]].values), 1))\n",
    "                 for feature in features)\n",
    "    interactions = ((f0, f1, -conditional_mutual_information(prob(dataset[[target, f0, f1]].values), 1, 2))\n",
    "                    for f0, f1 in itertools.permutations(features, 2))\n",
    "    return dimod.BinaryQuadraticModel(variables, interactions, 0, dimod.BINARY)\n",
    "\n",
    "\n",
    "def add_combination_penalty(bqm, k, penalty):\n",
    "    \"\"\"Create a new BQM with an additional penalty biased towards k-combinations\"\"\"\n",
    "    kbqm = dimod.generators.combinations(bqm.variables, k, strength=penalty)\n",
    "    kbqm.update(bqm)\n",
    "    return kbqm\n",
    "\n",
    "\n",
    "def mutual_information_feature_selection(dataset, features, target, num_reads=5000):\n",
    "    \"\"\"Run the MIFS algorithm on a QPU solver\"\"\"\n",
    "    \n",
    "    # Set up a QPU sampler that embeds to a fully-connected graph of all the variables\n",
    "    sampler = DWaveCliqueSampler()\n",
    "\n",
    "    # For each number of features, k, penalize selection of fewer or more features\n",
    "    selected_features = np.zeros((len(features), len(features)))\n",
    "\n",
    "    bqm = mutual_information_bqm(dataset, features, target)\n",
    "\n",
    "    # This ensures that the soltion will satisfy the constraints.\n",
    "    penalty = maximum_energy_delta(bqm)\n",
    "\n",
    "    for k in range(1, len(features) + 1):\n",
    "        kbqm = add_combination_penalty(bqm, k, penalty)\n",
    "        sample = sampler.sample(kbqm,\n",
    "                                label='Example - MI Feature Selection',\n",
    "                                num_reads=num_reads).first.sample\n",
    "        for fi, f in enumerate(features):\n",
    "            selected_features[k-1, fi] = sample[f]\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "def run_demo(dataset, target):\n",
    "    \"\"\"Compute MIFS for each value of k and visualize results\"\"\"\n",
    "\n",
    "    # Rank the MI between survival and every other variable\n",
    "    scores = {feature: mutual_information(prob(dataset[[target, feature]].values), 0)\n",
    "              for feature in set(dataset.columns) - {target}}\n",
    "\n",
    "    labels, values = zip(*sorted(scores.items(), key=lambda pair: pair[1], reverse=True))\n",
    "\n",
    "    # Plot the MI between survival and every other variable\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1.set_title(\"Mutual Information\")\n",
    "    ax1.set_ylabel('MI Between Survival and Feature')\n",
    "    plt.xticks(np.arange(len(labels)), labels, rotation=90)\n",
    "    plt.bar(np.arange(len(labels)), values)\n",
    "\n",
    "    # The Titanic dataset provides a familiar, intuitive example available in the public\n",
    "    # domain. In itself, however, it is not a good fit for solving by sampling. Run naively on\n",
    "    # this dataset, it finds numerous good solutions but is unlikely to find the exact optimal solution.\n",
    "    # There are many techniques for reformulating problems for the D-Wave system that can\n",
    "    # improve performance on various metrics, some of which can help narrow down good solutions\n",
    "    # to closer approach an optimal solution.\n",
    "    # This demo solves the problem for just the highest-scoring features.\n",
    "\n",
    "    # Select 8 features with the top MI ranking found above.\n",
    "    keep = 8\n",
    "\n",
    "    sorted_scores = sorted(scores.items(), key=lambda pair: pair[1], reverse=True)\n",
    "    dataset = dataset[[column[0] for column in sorted_scores[0:keep]] + [\"survived\"]]\n",
    "    features = sorted(list(set(dataset.columns) - {target}))\n",
    "    selected_features = mutual_information_feature_selection(dataset, features, target)\n",
    "    \n",
    "    # Plot the best feature set per number of selected features\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.set_title(\"Best Feature Selection\")\n",
    "    ax2.set_ylabel('Number of Selected Features')\n",
    "    ax2.set_xticks(np.arange(len(features)))\n",
    "    ax2.set_xticklabels(features, rotation=90)\n",
    "    ax2.set_yticks(np.arange(len(features)))\n",
    "    ax2.set_yticklabels(np.arange(1, len(features)+1))\n",
    "    # Set a grid on minor ticks\n",
    "    ax2.set_xticks(np.arange(-0.5, len(features)), minor=True)\n",
    "    ax2.set_yticks(np.arange(-0.5, len(features)), minor=True)\n",
    "    ax2.grid(which='minor', color='black')\n",
    "    ax2.imshow(selected_features, cmap=colors.ListedColormap(['white', 'red']))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the feature-engineered data into a pandas dataframe\n",
    "    # Data obtained from http://biostat.mc.vanderbilt.edu/DataSets\n",
    "    #demo_path = os.path.dirname(os.path.abspath(__file__))\n",
    "    demo_path = Path(__file__).parent.absolute()\n",
    "    data_path = os.path.join(demo_path, 'data', 'formatted_titanic.csv')\n",
    "    dataset = pd.read_csv(data_path)\n",
    "    run_demo(dataset, 'survived')\n",
    "    plots_path = os.path.join(demo_path, \"plots.png\")\n",
    "    plt.savefig(plots_path, bbox_inches=\"tight\")\n",
    "    print(\"Your plots are saved to {}\".format(plots_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7cb4c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ntpath.dirname(p)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8bc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
